#+TITLE: Blog Post 9

* Out of domain data experiments
  Our goal is to evaluate our model's ability to assign low credibility scores
  to predictions on out of domain data. In particular, we're interested in two
  approaches for evaluating this.

  One approach is holding out a specific label from training, then at test time
  asking the model to predict on the held-out data. We belive that this
  environment will simulate our original goal of community detection where the
  goal was to identify emergent communties that our model was not exposed to during
  training.

  The other approach is simply providing data from a similar, but different source.
  In the original DWAC paper, a baseline and DWAC model trained on the CIFAR
  image dataset was evaluated based on its credibility scores assigned to examples
  from the TinyImageNet dataset.

  For this blog post, we attemted experiments using out-of-domain data from the first of
  the two sources on the StackOverflow dataset. Here we would remove one of the 20 labels
  from training, then evaluate its credibility scores on predictions on the held out label.
  We do this for every label and a histogram of credibility scores assigned to predictions
  on the held out label for the baseline, DWAC, and ProtoDWAC models is located in the
  directory: [[../plots/05_22]]

  There's alot of plots to the first 2 categories (alphabetically):
  - ajax:
    - [[../plots/05_22/ajax_baseline_creds.pdf]]
    - [[../plots/05_22/ajax_dwac_creds.pdf]]
    - [[../plots/05_22/ajax_proto_creds.pdf]]
  - apache:
    - [[../plots/05_22/apache_baseline_creds.pdf]]
    - [[../plots/05_22/apache_dwac_creds.pdf]]
    - [[../plots/05_22/apache_proto_creds.pdf]]
  
  Here, it seems like there's a slight trend that the ProtoDwac and DWAC models are better
  at assigning low credibility scores to the out of domain predictions, but the trend is
  shaky. A big reason for this, we belive, is the lack of test examples when we're limiting
  ourselves to a single class. We're hoping that possibly holding out multiple labels at a time,
  or running many trials will allow us to get a better sense of how well our models compare
  in this setting.


* Visualization discussion
  2 blog posts ago ([[../blog_7.md]]), we included plots of PCA done to the output space of our different models. One trend we
  noticed is that the models with many prototypes tended to have clusters that were more separable in the PCA space, while the 
  original DWAC model did not. To gain a different view of the output space, we also performed t-SNE. We hypothesized that 
  t-SNE would be better at visualizing the output space as it can capture non linear relationships and clustering of the 
  features. However, to our surprise, the t-SNE visualizations of the stack overflow models with 256 prototypes is actually
  less separable than the PCA projection. We feel that we do not have enough information to hypothesize about the cause, but,
  we feel this implies that the variance in the output space is well expressed in a few dimensions, rather than in a strongly
  non linear fashion.

  All t-SNE plots are in [[../plots/05_22/tSNE]]
* References:
  - Dallas Card, Michael Zhang, and Noah A. Smith. Deep Weighted Averaging Classifiers. In Proceedings of FAT*, Atlanta, Georgia (2019).
