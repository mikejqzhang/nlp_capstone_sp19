#+TITLE: Blog Post 9

* Out of domain data experiments
  Our goal is to evaluate our model's ability to assign low credibility scores
  to predictions on out of domain data. In particular, we're interested in two
  approaches for evaluating this.

  One approach is holding out a specific label from training, then at test time
  asking the model to predict on the held-out data. We belive that this
  environment will simulate our original goal of community detection where the
  goal was to identify emergent communties that our model was not exposed to during
  training.

  The other approach is simply providing data from a similar, but different source.
  In the original DWAC paper, a baseline and DWAC model trained on the CIFAR
  image dataset was evaluated based on its credibility scores assigned to examples
  from the TinyImageNet dataset.

  For this blog post, we attemted experiments using out-of-domain data from the first of
  the two sources on the StackOverflow dataset. Here we would remove one of the 20 labels
  from training, then evaluate its credibility scores on predictions on the held out label.
  We do this for every label and a histogram of credibility scores assigned to predictions
  on the held out label for the baseline, DWAC, and ProtoDWAC models is located in the
  directory: [[../plots/05_22]]

  There's alot of plots to the first 2 categories (alphabetically):
  - ajax:
    - [[../plots/05_22/ajax_baseline_creds.pdf]]
    - [[../plots/05_22/ajax_dwac_creds.pdf]]
    - [[../plots/05_22/ajax_proto_creds.pdf]]
  - apache:
    - [[../plots/05_22/apache_baseline_creds.pdf]]
    - [[../plots/05_22/apache_dwac_creds.pdf]]
    - [[../plots/05_22/apache_proto_creds.pdf]]
  
  Here, it seems like there's a slight trend that the ProtoDwac and DWAC models are better
  at assigning low credibility scores to the out of domain predictions, but the trend is
  shaky. A big reason for this, we belive, is the lack of test examples when we're limiting
  ourselves to a single class. We're hoping that possibly holding out multiple labels at a time,
  or running many trials will allow us to get a better sense of how well our models compare
  in this setting.

* Refrences:
  - Dallas Card, Michael Zhang, and Noah A. Smith. Deep Weighted Averaging Classifiers. In Proceedings of FAT*, Atlanta, Georgia (2019).
