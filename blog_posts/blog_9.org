#+TITLE: Blog Post 9

* Out of domain data experiments
  Our goal is to evaluate our model's ability to assign low credibility scores
  to predictions on out of domain data. In particular, we're interested in two
  approaches for evaluating this.

  One approach is holding out a specific label from training, then at test time
  asking the model to predict on the held-out data. We belive that this
  environment will simulate our original goal of community detection where the
  goal was to identify emergent communties that our model was not exposed to during
  training.

  The other approach is simply providing data from a similar, but different source.
  In the original DWAC paper, a baseline and DWAC model trained on the CIFAR
  image dataset was evaluated based on its credibility scores assigned to examples
  from the TinyImageNet dataset.

  For this blog post, we attemted experiments using out-of-domain data from the first of
  the two sources on the StackOverflow dataset. Here we would remove one of the 20 labels
  from training, then evaluate its credibility scores on predictions on the held out label.
  We do this for every label and a histogram of credibility scores assigned to predictions
  on the held out label for the baseline, DWAC, and ProtoDWAC models is located in the
  directory: [[../plots/05_22]]

  There's alot of plots to the first 2 categories (alphabetically):
  - ajax:
    - [[../plots/05_22/ajax_baseline_creds.pdf]]
    - [[../plots/05_22/ajax_dwac_creds.pdf]]
    - [[../plots/05_22/ajax_proto_creds.pdf]]
  - apache:
    - [[../plots/05_22/apache_baseline_creds.pdf]]
    - [[../plots/05_22/apache_dwac_creds.pdf]]
    - [[../plots/05_22/apache_proto_creds.pdf]]
  
  Here, it seems like there's a slight trend that the ProtoDwac and DWAC models are better
  at assigning low credibility scores to the out of domain predictions, but the trend is
  shaky. A big reason for this, we belive, is the lack of test examples when we're limiting
  ourselves to a single class. We're hoping that possibly holding out multiple labels at a time,
  or running many trials will allow us to get a better sense of how well our models compare
  in this setting.

* Error analysis with confusion matrices
As a small experiment, we plotted the confusion matrices of our baseline model and DWAC model with 256 prototypes reported in the last blog post.
The baseline model has a dev accuracy of 86% and the prototype model has a dev accuracy of 86%. Since their accuracy was similar, we want to explore if they have good and bad performance on the same or different classes.
Confusion matrix of baseline model:
[[../plots/baseline.png]]
(row=reference, col=test)
Confusion matrix of prototype model:
[[../plots/prototype.png]]
From the matrices, we found that these two models had pretty similar performances on each class. Generally, they made the most mistakes by confusing "osx" with "cocoa". We think it was just because "osx" and "cocoa" are too classes that are difficult to differentiate. The matrices reflected some features of the dataset but not the models, which are what we are more focused on.

* Visualization discussion
  2 blog posts ago ([[blog_7.md]]), we included plots of PCA done to the output space of our different models. One trend we
  noticed is that the models with many prototypes tended to have clusters that were more separable in the PCA space, while the 
  original DWAC model did not. To gain a different view of the output space, we also performed t-SNE. We hypothesized that 
  t-SNE would be better at visualizing the output space as it can capture non linear relationships and clustering of the 
  features. However, to our surprise, the t-SNE visualizations of the stack overflow models with 256 prototypes is actually
  less separable than the PCA projection. We feel that we do not have enough information to hypothesize about the cause, but,
  we feel this implies that the variance in the output space is well expressed in a few dimensions, rather than in a strongly
  non linear fashion.

  All t-SNE plots are in [[../plots/05_22/tSNE]]
  
* New Model!
We also implemented the prototype model that uses the prototypes selected by us instead of randomly initializing them. We considered using bag-of-words to embed the data and using the determinant point process to sample prototypes from each class. However, we started from an easier approach this week-- randomly sampling data from each class and using them as prototypes. Following are the results of our experiments:

| Model                              | StackOverflow Dev Accuracy |
|------------------------------------|----------------------------|
| proto_dwac, 1 prototype/class     |                     0.4445 |
| proto_dwac, 16 prototypes/class    |                     0.8650 |
| proto_dwac, 64 prototypes/class    |                     0.8667 |
| proto_dwac, 256 prototypes/class   |                     0.8680 |

Comparing the results with the results of prototype model that randomly initializes the prototypes (reported in blog post 7), we found it has more stablely good performances among different number of prototypes. The previous model only achieve ~86% accuracy at several numbers of prototypes. However, the result of 1 prototype/class seems a little bit abnormal. We are going to explore this next week. 

* Next Step:
- First we want to explore the abnormal behavior we described above of our new model when having only 1 prototype for each class. During this time, we will also review our code for the new model and see if there's any potential mistake.
- Second, we still want to use bag-of-words to embed the data and the determinant point process to sample prototypes from each class. Therefore, we are going to try to implement this updated version of our new model and explore if it is going to have better performance than the current one.
- Since our prototype models seem to do well on clustering the data within the same class, we want to explore if it is capable of finding subclasses of the main classes if we select prototypes from each subclass and feed them in our model. Since we already implement the model that allows us to select the prototypes, we can force two classes to have the same label and feed prototypes from each of them in our model. If they form two clusters with data originally from the same classes, then we know our model could be use to detect the subclasses.

* References:
  - Dallas Card, Michael Zhang, and Noah A. Smith. Deep Weighted Averaging Classifiers. In Proceedings of FAT*, Atlanta, Georgia (2019).
